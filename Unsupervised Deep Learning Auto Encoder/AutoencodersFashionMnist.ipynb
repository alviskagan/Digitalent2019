{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AutoencodersFashionMnist.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7SuikKru01to","colab_type":"text"},"source":["**1. Load The Data**\n","\n","Keras comes with a library called datasets, which we can use to load datasets out of the box."]},{"cell_type":"code","metadata":{"id":"PVx16fCf0ptz","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt \n","from keras.datasets import fashion_mnist\n","from keras.layers import Input,Dense, Conv2D, MaxPooling2D, UpSampling2D\n","from keras.models import Model,Sequential\n","from keras.optimizers import RMSprop\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kmgAp7e7EgST","colab_type":"code","colab":{}},"source":["(train_X, train_Y),(test_X, test_Y) = fashion_mnist.load_data()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MhELfLAy1Wou","colab_type":"text"},"source":["Note that for this task, we don’t need training labels and testing labels. Our training images will both act as the input as well as the ground truth similar to the labels we have in classification task."]},{"cell_type":"markdown","metadata":{"id":"pOgzxxl71YGN","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"wBwKx8gO0_2N","colab_type":"text"},"source":["**2. Data Exploration**\n","\n","We shall first analyse how images in the dataset look like. Even though we know the dimension of the images by now, it is still worth the effort to analyze its programmatically, we might have to rescale the image pixels and resize the image:"]},{"cell_type":"code","metadata":{"id":"lsB-np3400Tw","colab_type":"code","outputId":"87e88cbd-e712-4c22-d87f-285965f40a40","executionInfo":{"status":"ok","timestamp":1565667779562,"user_tz":-420,"elapsed":1540,"user":{"displayName":"alviska galuh n","photoUrl":"https://lh4.googleusercontent.com/-SCIzWeHXp_I/AAAAAAAAAAI/AAAAAAAAAAU/dsg-SPKKsxA/s64/photo.jpg","userId":"15388036559215776599"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(\"Training set (images) shape :{shape}\".format(shape=train_X.shape))\n","print(\"Test set (images) shape :{shape}\".format(shape=test_X.shape) )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training set (images) shape :(60000, 28, 28)\n","Test set (images) shape :(10000, 28, 28)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x2pB1aX51OQr","colab_type":"text"},"source":["From the above output, we can see that the training data has shape of 60000 x 28 x 28 since there are 60,000 training samples each of 28 x 28 dimensions. Let’s create a dictionary that will have class names with their corresponding categorical class labels:"]},{"cell_type":"code","metadata":{"id":"eKDnNuaZ1Mlh","colab_type":"code","colab":{}},"source":["# Create dictionary of target classes\n","label_dict = {\n"," 0: 'T-shirt/top',\n"," 1: 'Trouser',\n"," 2: 'Pullover',\n"," 3: 'Dress',\n"," 4: 'Coat',\n"," 5: 'Sandal',\n"," 6: 'Shirt',\n"," 7: 'Sneake',\n"," 8: 'Bag',\n"," 9: 'Ankle Boot',\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"alEsS1o11cEa","colab_type":"text"},"source":["We will take a look a look at what the images in our dataset:"]},{"cell_type":"code","metadata":{"id":"z498TjMD1Tlg","colab_type":"code","outputId":"3662afb3-2374-42a2-87ec-0da5b0c6fccb","executionInfo":{"status":"ok","timestamp":1565667780115,"user_tz":-420,"elapsed":2060,"user":{"displayName":"alviska galuh n","photoUrl":"https://lh4.googleusercontent.com/-SCIzWeHXp_I/AAAAAAAAAAI/AAAAAAAAAAU/dsg-SPKKsxA/s64/photo.jpg","userId":"15388036559215776599"}},"colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["plt.figure(figsize=[5,5])\n"," \n","plt.subplot(121)\n","plt.imshow(train_X[1,:,:],cmap='gray')\n","plt.title('Ground Truth : {}'.format(train_Y[1]))\n"," \n","plt.subplot(122)\n","plt.imshow(test_X[1,:,:],cmap='gray')\n","plt.title('Ground Truth : {}'.format(test_Y[1]))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'Ground Truth : 2')"]},"metadata":{"tags":[]},"execution_count":52},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAATwAAACuCAYAAACr3LH6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEUIKAxc8Qkhh+D99nHmiKmMVEAAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 360x360 with 2 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"NWa2jGjS1gaV","colab_type":"text"},"source":["**3. Data Preprocessing**\n","\n","The images of the dataset are indeed grayscale images with pixel values ranging from 0 to 255 with a dimension of 28 x 28 so before we feed the data into the model it is very important to preprocess it. We will first convert each 28 x 28 image of train and test test set into a matrix of size 28 x 28 x 1, which we can feed into the network:"]},{"cell_type":"code","metadata":{"id":"ye_VAJbd1f0j","colab_type":"code","outputId":"569d5b52-5dca-4ad5-d9d3-6c71819ced68","executionInfo":{"status":"ok","timestamp":1565667780117,"user_tz":-420,"elapsed":2044,"user":{"displayName":"alviska galuh n","photoUrl":"https://lh4.googleusercontent.com/-SCIzWeHXp_I/AAAAAAAAAAI/AAAAAAAAAAU/dsg-SPKKsxA/s64/photo.jpg","userId":"15388036559215776599"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_X = train_X.reshape(-1, 28, 28, 1)\n","test_X = test_X.reshape(-1, 28, 28, 1)\n","train_X.shape, test_X.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((60000, 28, 28, 1), (10000, 28, 28, 1))"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"t4YF4RNz2hTp","colab_type":"text"},"source":["The data right now is in an int8 format, so before we feed it into the network we need to convert it type to float32, and we also have to rescale the pixel values in range 0-1 inclusive."]},{"cell_type":"code","metadata":{"id":"zjdgobPt2gEQ","colab_type":"code","colab":{}},"source":["train_X = train_X.astype('float32')/255.\n","test_X = test_X.astype('float32')/255."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XEf_ZAPr2me8","colab_type":"text"},"source":["After all of this , it’s import to partition the data. In order for our model to generalize well, we split the training data into two parts: a training and a validation set. We will train our model on 80& of the data and validate in on 20% of the remaining training data."]},{"cell_type":"code","metadata":{"id":"o9I4xuqp2lha","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","train_X, valid_X, train_ground, valid_ground = train_test_split(train_X,\n","                                                                train_X,\n","                                                               test_size=0.2,\n","                                                               random_state=13)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jVbaiQYu2v-Y","colab_type":"text"},"source":["Now we are all set to define the network and feed the data into the network!"]},{"cell_type":"markdown","metadata":{"id":"BRkjGEDl2x7-","colab_type":"text"},"source":["**4. The Convolutional Autoencoder**\n","\n","The images are of size 28 x 28 x 1 or a 784-dimensional vector. You convert the image matrix to an array, rescale it between 0 and 1, reshape it so that it’s of size 28 x 28 x 1, and feed this as an input to the network.\n","\n","Also, you will use a batch size of 128 using a higher batch size of 256 or 512 is also preferable it all depends on the system you train your model. It contributes heavily in determining the learning parameters and affects the prediction accuracy. You will train your network for 50 epochs"]},{"cell_type":"code","metadata":{"id":"aIliZxDp2r2e","colab_type":"code","colab":{}},"source":["batch_size = 128\n","epochs = 50\n","inChannel = 1\n","x, y = 28, 28\n","input_img = Input(shape=(x,y,inChannel))\n"," "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PWGtETe83HHm","colab_type":"text"},"source":["As discussed before, the autoencoder is divided into two parts: encoder and decoder."]},{"cell_type":"markdown","metadata":{"id":"V4FQC2WX3JHW","colab_type":"text"},"source":["Encoder\n","\n"," \n","\n","*      The first layer will have 32 filters of size 3 x 3, followed by a downsampling (max-pooling) layer\n","*       The second layer will have 64 filters of size 3 x 3, followed by a another downsampling layer\n","*       The final layer of encoder will have 128 filters of size 3 x 3\n","\n","Decoder\n","\n","\n","*       The first layer will have 128 filters of size 3 x 3, followed by a upsampling layer\n","*       The second layer will have 64 filters of size 3 x 3, followed by a another upsampling layer\n","*       The final layer of encoder will have 1filters of size 3 x 3\n","\n","Let’s define the model."]},{"cell_type":"code","metadata":{"id":"CUmGHSe727rT","colab_type":"code","colab":{}},"source":["def autoencoder(input_img):\n","  # encoder\n","  conv1 = Conv2D(32, (3,3), activation='relu', padding='same')(input_img) # 28 x 28 x 32\n","  pool1 = MaxPooling2D(pool_size=(2,2))(conv1) # 14 x 14 x 32\n","  conv2 = Conv2D(64, (3,3), activation='relu', padding='same')(pool1)# 14 x 14 x 64\n","  pool2 = MaxPooling2D(pool_size=(2,2))(conv2) # 7 x 7 x 64\n","  conv3 = Conv2D(128, (3,3), activation='relu', padding='same')(pool2) # 7 x 7 x 128\n","  \n","  \n","  # decoder\n","  conv4 = Conv2D(128, (3,3), activation='relu', padding='same')(conv3) # 7 x 7 x 128\n","  up1 = UpSampling2D((2,2))(conv4) # 14 x 14 x 128\n","  conv5 = Conv2D(64, (3,3), activation='relu', padding='same')(up1)# 14 x 14 x 64\n","  up2 = UpSampling2D((2,2))(conv5) # 28 x 28 x 64\n","  decoded = Conv2D(1, (3,3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n","  \n","  return decoded"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxAd_Lmg3iDA","colab_type":"code","colab":{}},"source":["autoencoder = Model(input_img, autoencoder(input_img))\n","autoencoder.compile(loss='mean_squared_error',optimizer=RMSprop())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X4zKwely3knf","colab_type":"text"},"source":["we can visualize the architecture of our model!"]},{"cell_type":"code","metadata":{"id":"SUg7kwyX3j5m","colab_type":"code","outputId":"51b37dc6-c34b-4ce8-8b02-0ef29cea2331","executionInfo":{"status":"ok","timestamp":1565667780758,"user_tz":-420,"elapsed":2611,"user":{"displayName":"alviska galuh n","photoUrl":"https://lh4.googleusercontent.com/-SCIzWeHXp_I/AAAAAAAAAAI/AAAAAAAAAAU/dsg-SPKKsxA/s64/photo.jpg","userId":"15388036559215776599"}},"colab":{"base_uri":"https://localhost:8080/","height":510}},"source":["autoencoder.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_8 (InputLayer)         (None, 28, 28, 1)         0         \n","_________________________________________________________________\n","conv2d_25 (Conv2D)           (None, 28, 28, 32)        320       \n","_________________________________________________________________\n","max_pooling2d_9 (MaxPooling2 (None, 14, 14, 32)        0         \n","_________________________________________________________________\n","conv2d_26 (Conv2D)           (None, 14, 14, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_10 (MaxPooling (None, 7, 7, 64)          0         \n","_________________________________________________________________\n","conv2d_27 (Conv2D)           (None, 7, 7, 128)         73856     \n","_________________________________________________________________\n","conv2d_28 (Conv2D)           (None, 7, 7, 128)         147584    \n","_________________________________________________________________\n","up_sampling2d_7 (UpSampling2 (None, 14, 14, 128)       0         \n","_________________________________________________________________\n","conv2d_29 (Conv2D)           (None, 14, 14, 64)        73792     \n","_________________________________________________________________\n","up_sampling2d_8 (UpSampling2 (None, 28, 28, 64)        0         \n","_________________________________________________________________\n","conv2d_30 (Conv2D)           (None, 28, 28, 1)         577       \n","=================================================================\n","Total params: 314,625\n","Trainable params: 314,625\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EqnnYjz7Csg_","colab_type":"text"},"source":["It’s finally time to train the model with Keras’ fit() function! The model trains for 50 epochs. The fit() function will return a history object; By storing the result of this function in fashion_train, we can use it later to plot the loss function plot between training and validation which will help you to analyze your model’s performance visually."]},{"cell_type":"code","metadata":{"id":"7KFFvZqzCgh0","colab_type":"code","outputId":"912d17b1-1c81-48e0-b62b-d9771ee2240b","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["autoencoder_train = autoencoder.fit(train_X, train_ground, batch_size=batch_size, epochs=epochs, verbose=2, validation_data=(valid_X, valid_ground))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 48000 samples, validate on 12000 samples\n","Epoch 1/50\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a4N_bAa-C0V4","colab_type":"text"},"source":["We can now visualize the result by plotting them!"]},{"cell_type":"code","metadata":{"id":"NtYE7fZTCwBi","colab_type":"code","colab":{}},"source":["loss = autoencoder_train.history['loss']\n","val_loss = autoencoder_train.history['val_loss']\n","epochs = range(epochs)\n","plt.figure()\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VOpDh8CYDNWr","colab_type":"text"},"source":["Finally, we can see that the validation loss and the training loss both are in sync. It shows that our model is not overfitting: the validation loss is decreasing and not increasing, and there rarely any gap between training and validation loss.\n","\n","Therefore, we can say that our model’s generalization capability is good.\n","\n","Finally, it’s time to reconstruct the test images using the predict() function of Keras and see how well our model is able reconstruct on the test data."]},{"cell_type":"markdown","metadata":{"id":"Y6EpPEh-DRgt","colab_type":"text"},"source":["**5. Predicting on Test Data**\n","\n","We will be predicting the trained model on the complete 10,000 test images and plot few of the reconstructed images to visualize how well our model is able to reconstruct the test images."]},{"cell_type":"code","metadata":{"id":"ukP-WrrEDQgf","colab_type":"code","colab":{}},"source":["pred = autoencoder.predict(test_X)\n"," \n","plt.figure(figsize=(20,4))\n","print(\"Test images\")\n","for i in range(10):\n","  plt.subplot(2, 10, i+1)\n","  plt.imshow(test_X[i,...,0],cmap='gray')\n","  curr_lbl = test_Y[i]\n","  plt.title(\"(Label: \"+str(label_dict[curr_lbl]) + \")\")\n","  \n","plt.show()\n","plt.figure(figsize=(20,4))\n","print(\"Reconstruct of Test images\")\n","for i in range(10):\n","  plt.subplot(2,10,i+1)\n","  plt.imshow(pred[i,...,0],cmap='gray')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q5iU7DI_ELt_","colab_type":"text"},"source":["From the above figures, we can observe that your model did a fantastic job in reconstructing the test images that you predicted using the model. At least visually speaking, the test and the reconstructed images look almost exactly similar."]}]}